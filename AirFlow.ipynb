{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNuIoLMOCxRkh4dFmuLBi9A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Airflow is a platform (open-source) for programming and managing data workflows. It provides a scheduler, handles error handling, and offers reporting features for these workflows. The tasks within a workflow are defined using a DAG."],"metadata":{"id":"z8IC54rzNTV5"}},{"cell_type":"markdown","source":["# Defining a simple DAG"],"metadata":{"id":"KdoZSWd8NUzl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CEZA3KqHNSWz"},"outputs":[],"source":["# Import the DAG object\n","from airflow import DAG\n","\n","# Define the default_args dictionary\n","default_args = {\n","  'owner': 'dsmith',\n","  'start_date': datetime(2023, 1, 14),\n","  'retries': 2\n","}\n","\n","# Instantiate the DAG object\n","with DAG('example_etl', default_args=default_args) as etl_dag:\n","  pass"]},{"cell_type":"markdown","source":["# Starting the Airflow webserver"],"metadata":{"id":"3m6GLD5WP9Vn"}},{"cell_type":"code","source":["airflow webserver -p 9090"],"metadata":{"id":"V8IDreOzP9hj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cli_scheduler : event represents the presence of the Airflow scheduler process, a default process for our installation.\n","# cli_webserver : event indicates the web UI is running and available\n","# cli_worker    : referring to the command-line interface (CLI) for running an Airflow worker\n"],"metadata":{"id":"c7YXynlMbo9Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Defining an operator.\n","Using a `BashOperator` task as a practice example"],"metadata":{"id":"Dw0WhFTefCOi"}},{"cell_type":"code","source":["# Import the BashOperator\n","from airflow.operators.bash import BashOperator\n","\n","with DAG(dag_id=\"test_dag\", default_args={\"start_date\": \"2024-01-01\"}) as analytics_dag:\n","  # Define the BashOperator\n","  cleanup = BashOperator(\n","      task_id='cleanup_task',\n","      # Define the bash_command\n","      bash_command='cleanup.sh',\n","  )"],"metadata":{"id":"XFwChgmlcHAS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Multiple BashOperators"],"metadata":{"id":"9qWPzg83eM1N"}},{"cell_type":"code","source":["# Define a second operator to run the `consolidate_data.sh` script\n","consolidate = BashOperator(\n","    task_id='consolidate_task',\n","    bash_command='consolidate_data.sh'\n","    )\n","\n","# Define a final operator to execute the `push_data.sh` script\n","push_data = BashOperator(\n","    task_id='pushdata_task',\n","    bash_command='push_data.sh'\n","    )"],"metadata":{"id":"AGjAkaYzeM_o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define order of BashOperators"],"metadata":{"id":"G4zbaVsteS_N"}},{"cell_type":"code","source":["# Define a new pull_sales task\n","pull_sales = BashOperator(\n","    task_id='pullsales_task',\n","    bash_command='wget https://salestracking/latestinfo?json'\n",")\n","\n","# Set pull_sales to run prior to cleanup\n","pull_sales >> cleanup\n","\n","# Configure consolidate to run after cleanup\n","cleanup >> consolidate\n","\n","# Set push_data to run last\n","consolidate >> push_data"],"metadata":{"id":"exyDGhnVeTJG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Troubleshooting DAG dependencies\n"],"metadata":{"id":"PfqueWUVlxuv"}},{"cell_type":"code","source":["# airflow dags\n","# airflow dags report\n","# cat workspace/dags/codependent.py (to view python code)"],"metadata":{"id":"DlLU1nCQlx4r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Using the PythonOperator\n"],"metadata":{"id":"L-FmhaXQnGIk"}},{"cell_type":"code","source":["# Define the method\n","def pull_file(URL, savepath):\n","    r = requests.get(URL)\n","    with open(savepath, 'wb') as f:\n","        f.write(r.content)\n","    # Use the print method for logging\n","    print(f\"File pulled from {URL} and saved to {savepath}\")\n","\n","# Import the PythonOperator class\n","from airflow.operators.python import PythonOperator\n","\n","# Create the task\n","pull_file_task = PythonOperator(\n","    task_id='pull_file',\n","    # Add the callable\n","    python_callable=pull_file,\n","    # Define the arguments\n","    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'}\n",")"],"metadata":{"id":"VO3E6S5WnGSf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add another Python task\n","parse_file_task = PythonOperator(\n","    task_id='parse_file',\n","    # Set the function to call\n","    python_callable=parse_file,\n","    # Add the arguments\n","    op_kwargs={'inputfile':'latestsales.json', 'outputfile':'parsedfile.json'},\n",")\n"],"metadata":{"id":"FpGURjddsmN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import the Operator\n","from airflow.operators.email import EmailOperator\n","\n","# Define the task\n","email_manager_task = EmailOperator(\n","    task_id='email_manager',\n","    to='manager@datacamp.com',\n","    subject='Latest sales JSON',\n","    html_content='Attached is the latest sales JSON file as requested.',\n","    files='parsedfile.json',\n","    dag=process_sales_dag\n",")\n","\n","# Set the order of tasks\n","pull_file_task >> parse_file_task >> email_manager_task"],"metadata":{"id":"XKhNg6zedBqA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Schedule a DAG via Python\n","- Set the start date of the DAG to November 1, 2023\n","- Configure the retry_delay to 20 minutes\n","- Useing the cron syntax to configure a schedule of every Wednesday at 12:30pm"],"metadata":{"id":"1cwJuaXtd_71"}},{"cell_type":"code","source":["# Update the scheduling arguments as defined\n","default_args = {\n","  'owner': 'Engineering',\n","  'start_date': datetime(2023, 11, 1),\n","  'email': ['airflowresults@datacamp.com'],\n","  'email_on_failure': False,\n","  'email_on_retry': False,\n","  'retries': 3,\n","  'retry_delay': timedelta(minutes=20)\n","}\n","\n","dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')"],"metadata":{"id":"lTTkFhrZeAGx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Defining an SLA"],"metadata":{"id":"42vXPH_UT9wf"}},{"cell_type":"code","source":["# Import the timedelta object\n","from datetime import timedelta\n","\n","# Create the dictionary entry\n","default_args = {\n","  'start_date': datetime(2024, 1, 20),\n","  'sla': timedelta(minutes=30)\n","}\n","\n","# Add to the DAG\n","test_dag = DAG('test_workflow', default_args=default_args, schedule_interval=None)"],"metadata":{"id":"0jlPxgqrT9-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import the timedelta object\n","from datetime import timedelta\n","\n","test_dag = DAG('test_workflow', start_date=datetime(2024,1,20), schedule_interval=None)\n","\n","# Create the task with the SLA\n","task1 = BashOperator(task_id='first_task',\n","                     sla=timedelta(hours=3),\n","                     bash_command='initialize_data.sh',\n","                     dag=test_dag)"],"metadata":{"id":"ls3-M1KfU6re"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate and email a report"],"metadata":{"id":"Q2UWmm8CUQtJ"}},{"cell_type":"code","source":["# Define the email task\n","email_report = EmailOperator(\n","        task_id='email_report',\n","        to='airflow@datacamp.com',\n","        subject='Airflow Monthly Report',\n","        html_content=\"\"\"Attached is your monthly workflow report - please refer to it for more detail\"\"\",\n","        files=['monthly_report.pdf'],\n","        dag=report_dag\n",")\n","\n","# Set the email task to run after the report is generated\n","email_report << generate_report"],"metadata":{"id":"_H_xsEQPUQ2j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Adding status emails\n"],"metadata":{"id":"Tfx8e8Njck2j"}},{"cell_type":"code","source":["from airflow import DAG\n","from airflow.operators.bash import BashOperator\n","from airflow.sensors.filesystem import FileSensor\n","from datetime import datetime\n","\n","default_args={\n","    'email': ['irflowalerts@datacamp.com'],\n","    'email_on_failure': True,\n","    'email_on_success': True\n","}\n","\n","report_dag = DAG(\n","    dag_id = 'execute_report',\n","    schedule_interval = \"0 0 * * *\",\n","    default_args=default_args\n",")\n","\n","precheck = FileSensor(\n","    task_id='check_for_datafile',\n","    filepath='salesdata_ready.csv',\n","    start_date=datetime(2023,2,20),\n","    mode='reschedule',\n","    dag=report_dag)\n","\n","generate_report_task = BashOperator(\n","    task_id='generate_report',\n","    bash_command='generate_report.sh',\n","    start_date=datetime(2023,2,20),\n","    dag=report_dag\n",")\n","\n","precheck >> generate_report_task\n"],"metadata":{"id":"Ew54WvDsclDc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Creating a templated BashOperator"],"metadata":{"id":"lgx7hYvWowW1"}},{"cell_type":"code","source":["from airflow import DAG\n","from airflow.operators.bash import BashOperator\n","from datetime import datetime\n","\n","default_args = {\n","  'start_date': datetime(2023, 4, 15),\n","}\n","\n","cleandata_dag = DAG('cleandata',\n","                    default_args=default_args,\n","                    schedule_interval='@daily')\n","\n","# Create a templated command to execute\n","# 'bash cleandata.sh datestring'\n","templated_command = \"bash cleandata.sh {{ ds_nodash}} \"\n","\n","# Modify clean_task to use the templated command\n","clean_task = BashOperator(task_id='cleandata_task',\n","                          bash_command=templated_command,\n","                          dag=cleandata_dag)\n"],"metadata":{"id":"C9buKZ_JowtJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Templates with multiple arguments\n"],"metadata":{"id":"2bWAAcRSpL0N"}},{"cell_type":"code","source":["from airflow import DAG\n","from airflow.operators.bash import BashOperator\n","from datetime import datetime\n","\n","default_args = {\n","  'start_date': datetime(2023, 4, 15),\n","}\n","\n","cleandata_dag = DAG('cleandata',\n","                    default_args=default_args,\n","                    schedule_interval='@daily')\n","\n","# Modify the templated command to handle a\n","# second argument called filename.\n","templated_command = \"\"\"\n","  bash cleandata.sh {{ ds_nodash }} {{ params.filename }}\n","\"\"\"\n","\n","# Modify clean_task to pass the new argument\n","clean_task = BashOperator(task_id='cleandata_task',\n","                          bash_command=templated_command,\n","                          params={'filename': 'salesdata.txt'},\n","                          dag=cleandata_dag)\n","\n","# Create a new BashOperator clean_task2\n","clean_task2 = BashOperator(task_id='cleandata_task2',\n","                           bash_command=templated_command,\n","                           params={'filename': 'supportdata.txt'},\n","                           dag=cleandata_dag)\n","\n","# Set the operator dependencies\n","clean_task2 << clean_task\n"],"metadata":{"id":"q7iX5gBlpLjn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Using lists with templates\n","- Modify the templated_command to iterate over a list of filenames.\n","- Pass the filelist to the templated command in the operator.\n","\n"],"metadata":{"id":"Bhvt91_Qqcb6"}},{"cell_type":"code","source":["from airflow import DAG\n","from airflow.operators.bash import BashOperator\n","from datetime import datetime\n","\n","filelist = [f'file{x}.txt' for x in range(30)]\n","\n","default_args = {\n","  'start_date': datetime(2020, 4, 15),\n","}\n","\n","cleandata_dag = DAG('cleandata',\n","                    default_args=default_args,\n","                    schedule_interval='@daily')\n","\n","# Modify the template to handle multiple files in a\n","# single run.\n","templated_command = \"\"\"\n","  <% for filename in params.filenames %>\n","  bash cleandata.sh {{ ds_nodash }} {{ filename }};\n","  <% endfor %>\n","\"\"\"\n","\n","# Modify clean_task to use the templated command\n","clean_task = BashOperator(task_id='cleandata_task',\n","                          bash_command=templated_command,\n","                          params={'filenames': filelist},\n","                          dag=cleandata_dag)\n"],"metadata":{"id":"XcA2T9_qqctl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"LJ5WVI73vNtW"}},{"cell_type":"code","source":[],"metadata":{"id":"jEBWXstMvNSx"},"execution_count":null,"outputs":[]}]}